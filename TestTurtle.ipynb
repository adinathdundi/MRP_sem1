{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07663308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query_graph.py', 'turtle_converter.py', 'TestTurtle.ipynb', 'README.md', '.git', 'data', 'src']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List all files in the current directory\n",
    "files = os.listdir('.')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3669183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 44892 triples.\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "# Load the Turtle file into an rdflib graph\n",
    "graph = Graph()\n",
    "graph.parse(\"data/sphn100.ttl\", format=\"turtle\")\n",
    "\n",
    "\n",
    "print(f\"Graph has {len(graph)} triples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee4767a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation complete. Saved to data/sphn100.ttl.\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "from rdflib.namespace import RDF\n",
    "\n",
    "# Define a namespace for your data\n",
    "SPHN = Namespace(\"http://example.org/sphn#\")\n",
    "\n",
    "# File paths\n",
    "input_file_path = \"data/sphn100.txt\"\n",
    "output_file_path = \"data/sphn100.ttl\"\n",
    "\n",
    "# Create a new graph\n",
    "new_graph = Graph()\n",
    "\n",
    "# Read the triples from the text file\n",
    "with open(input_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        subject, predicate, obj = line.strip().split(\",\")\n",
    "        \n",
    "        # Add the triple to the graph\n",
    "        new_graph.add((\n",
    "            URIRef(f\"http://example.org/{subject.strip()}\"),\n",
    "            URIRef(f\"http://example.org/{predicate.strip()}\") if predicate != \"a\" else RDF.type,\n",
    "            URIRef(SPHN[obj.strip()]) if obj.startswith(\"sphn#\") else Literal(obj.strip())\n",
    "        ))\n",
    "\n",
    "# Serialize the graph to Turtle format and save it\n",
    "new_graph.serialize(destination=output_file_path, format=\"turtle\")\n",
    "print(f\"Transformation complete. Saved to {output_file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d48e03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# gcn-matching\n",
       "\n",
       "Matching nodes in a knowledge graph using Graph Convolutional Networks and investigating the interplay between formal semantics and GCNs.\n",
       "\n",
       "A detailed description of the motivation and the algorithms is available in [the related paper](https://doi.org/10.3233/SW-210452).\n",
       "\n",
       "## Citing\n",
       "\n",
       "When citing, please use the following reference:\n",
       "\n",
       "Pierre Monnin, Chedy RaĂŻssi, Amedeo Napoli, Adrien Coulet: Discovering alignment relations with Graph Convolutional Networks: A biomedical case study. Semantic Web 13(3): 379-398 (2022)\n",
       "\n",
       "```\n",
       "@article{monninRNC22,\n",
       "  author    = {Pierre Monnin and\n",
       "               Chedy Ra{\\\"{\\i}}ssi and\n",
       "               Amedeo Napoli and\n",
       "               Adrien Coulet},\n",
       "  title     = {Discovering alignment relations with Graph Convolutional Networks:\n",
       "               {A} biomedical case study},\n",
       "  journal   = {Semantic Web},\n",
       "  volume    = {13},\n",
       "  number    = {3},\n",
       "  pages     = {379--398},\n",
       "  year      = {2022},\n",
       "  url       = {https://doi.org/10.3233/SW-210452},\n",
       "  doi       = {10.3233/SW-210452}\n",
       "}\n",
       "```\n",
       "\n",
       "## Scripts\n",
       "\n",
       "### 1. Query similarity set\n",
       "\n",
       "* In ``query_simset.py``\n",
       "* Retrieve individuals to match (instances of classes in ``individuals-classes`` in the JSON configuration file)\n",
       "* Retrieve similarity links between these individuals (to use in train/valid/test sets).\n",
       "  * Similarity links are described in ``similarity-links`` in the JSON configuration file\n",
       "  * When having the link ``(url1, url2)``, we do not add ``(url2, url1)`` for symmetric predicates to avoid the symmetry\n",
       "  bias in training\n",
       "\n",
       "### 2. Query graph\n",
       "\n",
       "* In ``query_graph.py``\n",
       "* Retrieve the adjacency of the RDF graph (except similarity links previously retrieved in 1.)\n",
       "* Retrieve predicates and their inverses (or symmetry)\n",
       "* **Must be used with the cache manager resulting from the previous step**\n",
       "\n",
       "### 3. Similarity analysis\n",
       "\n",
       "* In ``similarity_analysis.py``\n",
       "* Output PDF files with histograms depicting the size of similarity clusters and number of them for each model (computed\n",
       "based on the similarity links considered by each model)\n",
       "* Similarity clusters for each model are computed over all similarity links indifferently considered by the model in an\n",
       " undirected (symmetry) and transitive fashion\n",
       "\n",
       "### 4. N Fold Split\n",
       "\n",
       "* In ``n_fold_split.py``\n",
       "* Output a n-fold split of similarity links (after shuffling)\n",
       "\n",
       "### 5. Transform graph\n",
       "\n",
       "* In ``transform_graph.py``\n",
       "* Output a DGL graph from the given RDF graph applying one of the following transformations:\n",
       "  * G<sub>0</sub>: RDF graph + adding an abstract inverse for each predicate\n",
       "  * G<sub>1</sub>: RDF graph after owl:sameAs edges contraction (only considering canonical nodes)\n",
       "  * G<sub>2</sub>: RDF graph with consideration of inverse predicates / symmetry (to avoid adding abstract inverses when\n",
       "  not needed)\n",
       "  * G<sub>3</sub>: RDF graph with links added based on the hierarchy of predicates: if (a, rel<sub>1</sub>, b) and\n",
       "  (rel<sub>1</sub>, subPropertyOf, rel<sub>2</sub>), we add (a, rel<sub>2</sub>, b)\n",
       "  * G<sub>4</sub>: RDF graph with ``rdf:type`` links added based on the hierarchy of classes: if (a, type, b) and (b,\n",
       "  subClassOf, c), we add (a, type, c)\n",
       "  * G<sub>5</sub>:  all transformations of G<sub>1</sub> to G<sub>4</sub>\n",
       "* The graph is limited to the considered neighborhood of individuals to match based on the number of layers\n",
       "\n",
       "### 6. Learning\n",
       "\n",
       "* In ``learning.py``\n",
       "* Output a python dict where each key is the index of the test fold and contains:\n",
       "  * ``logits_history``: python list associating an epoch with its logits\n",
       "  * ``train_loss_history``: python list associating an epoch with its train loss\n",
       "  * ``val_loss_history``: python list associating an epoch with its validation loss\n",
       "  * ``test_loss_history``: python list associating an epoch with its test loss\n",
       "  * ``temperature_history``: python list associating an epoch with its temperature\n",
       "  * ``model``: python list associating an epoch with the parameters of the GCN model\n",
       "\n",
       "### 7. Clustering analysis\n",
       "\n",
       "* In ``clustering_analysis.py``\n",
       "* Output for each fold:\n",
       "  * A distance analysis based on all links, links whose nodes are in the training set, in the validation set, and in the test set\n",
       "  * A UMAP projection computed on all nodes and displayed for all nodes, nodes in the training set, in the validation set and in the test set.\n",
       "  Only ``--umap-colors`` similarity clusters are colored (starting at the biggest ones). Only similarity clusters containing more than ``--umap-size`` nodes are displayed (0 to disable)\n",
       "  * A plot of the training, validation, and test losses\n",
       "  * A plot of the temperature\n",
       "\n",
       "## Dependencies\n",
       "\n",
       "* Python3.7\n",
       "* tqdm\n",
       "* requests\n",
       "* pytorch\n",
       "* dgl\n",
       "* matplotlib\n",
       "* scikit-learn\n",
       "* umap-learn\n",
       "* pynndescent\n",
       "\n",
       "## Experiments\n",
       "\n",
       "### Models\n",
       "\n",
       "(called gold clusterings in the preprint)\n",
       "\n",
       "| Similarity links | owl:sameAs | skos:closeMatch | skos:relatedMatch | skos:related | skos:broadMatch |\n",
       "|---------|---------|---------|---------|---------|---------|\n",
       "| Properties | T / S | T / S | nT / S | nT / S | T / nS |\n",
       "| M<sub>0</sub> | X | X | X | X | X |\n",
       "| M<sub>1</sub> | X | X | X | X | |\n",
       "| M<sub>2</sub> | X |  |  |  |  |\n",
       "| M<sub>3</sub> |Â  | X |  |  |  |\n",
       "| M<sub>4</sub> |Â  |  | X |  |  |\n",
       "| M<sub>5</sub> |Â  |  |  | X |  |\n",
       "| M<sub>6</sub> |Â  |  |  |  | X |\n",
       "\n",
       "* **T**: transitivity\n",
       "* **S**: symmetry\n",
       "* **nT** : non-transitivity\n",
       "* **nS** : non-symmetry\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Read the contents of README.md\n",
    "with open(\"README.md\", \"r\") as file:\n",
    "    readme_content = file.read()\n",
    "\n",
    "\n",
    "# Display the contents as markdown\n",
    "Markdown(readme_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
